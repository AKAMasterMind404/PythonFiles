# -*- coding: utf-8 -*-
"""CustomerHapinessMeter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MES-EhTdRQQFjF88zR-iRQhifMwhfZWZ

# 1. Importing libraries and loading Data
"""

from pkg_resources import get_distribution

"""### 1.1 Installing necessary libraries

"""

# Tejas Model
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import numpy as np
import emoji
import re
import string
from transformers import TFBertModel, BertTokenizerFast, BertConfig
import tensorflow as tf
from keras.layers import Input, Dropout, Dense, BatchNormalization
from keras.models import Model
from sklearn.metrics import f1_score, recall_score, precision_score
from tensorflow.keras.utils import plot_model
from keras.initializers import TruncatedNormal
import keras.backend as K

"""### 1.3 Helper Functions"""


def idx2class(idx_list):
    """
    This function converts a list of class indices to a list of class labels.

    Parameters
    ----------
    idx_list : list
        List of class indices.
    
    Returns
    -------
    class_list : list
        List of class labels.
    """
    arr = []
    for i in idx_list:
        arr.append(labels[int(i)])
    return arr


def EmotionMapping(list_of_emotions):
    list = []
    for i in list_of_emotions:
        if i in ekman_map['anger']:
            list.append('anger')
        if i in ekman_map['fear']:
            list.append('fear')
        if i in ekman_map['joy']:
            list.append('joy')
        if i in ekman_map['sadness']:
            list.append('sadness')
        if i in ekman_map['surprise']:
            list.append('surprise')
        if i == 'neutral':
            list.append('neutral')
    return list


def SentimentMapping(list_of_emotions):
    list = []
    for i in list_of_emotions:
        if i in sentiment_map['positive']:
            list.append('positive')
        if i in sentiment_map['negative']:
            list.append('negative')
        if i in sentiment_map['ambiguous']:
            list.append('ambiguous')
    return list


"""### 1.4 Loading data"""

train_url = 'https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/train.tsv'
valid_url = 'https://github.com/google-research/google-research/raw/master/goemotions/data/dev.tsv'
test_url = 'https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/train.tsv'

train_df = pd.read_csv(train_url, sep='\t', encoding='utf-8',
                       names=['text', 'emotion', 'annotator'], header=None)
valid_df = pd.read_csv(valid_url, sep='\t', encoding='utf-8',
                       names=['text', 'emotion', 'annotator'], header=None)
test_df = pd.read_csv(test_url, sep='\t', encoding='utf-8',
                      names=['text', 'emotion', 'annotator'], header=None)

train_df.head(10)

"""### 1.5 Preprocessing

Column 2 "annotator" is unnecessary, so we can drop it.
"""

train_df.drop('annotator', axis=1, inplace=True)
valid_df.drop('annotator', axis=1, inplace=True)
test_df.drop('annotator', axis=1, inplace=True)

"""Dictionaries for mapping emotions to indices and vice versa. 

The variable `ekman_map` is used to map 27 emotions to 7 emotions. This is done to reduce the number of classes.

The 27 emotions can also be mapped to the 3 emotions using the `sentiment_map` dictionary for sentiment analysis tasks.

"""

labels = {
    0: 'admiration',
    1: 'amusement',
    2: 'anger',
    3: 'annoyance',
    4: 'approval',
    5: 'caring',
    6: 'confusion',
    7: 'curiosity',
    8: 'desire',
    9: 'disappointment',
    10: 'disapproval',
    11: 'disgust',
    12: 'embarrassment',
    13: 'excitement',
    14: 'fear',
    15: 'gratitude',
    16: 'grief',
    17: 'joy',
    18: 'love',
    19: 'nervousness',
    20: 'optimism',
    21: 'pride',
    22: 'realization',
    23: 'relief',
    24: 'remorse',
    25: 'sadness',
    26: 'surprise',
    27: 'neutral'
}

ekman_map = {
    'anger': ['anger', 'annoyance', 'disapproval', 'confusion', 'disgust'],
    'fear': ['fear', 'nervousness'],
    'joy': ['joy', 'amusement', 'approval', 'excitement', 'gratitude', 'love', 'optimism', 'relief', 'pride',
            'admiration', 'desire', 'caring'],
    'sadness': ['sadness', 'disappointment', 'embarrassment', 'grief', 'remorse'],
    'surprise': ['surprise', 'realization', 'confusion', 'curiosity'],
    'neutral': ['neutral']
}

sentiment_map = {
    "positive": ["amusement", "excitement", "joy", "love", "desire", "optimism", "caring", "pride", "admiration",
                 "gratitude", "relief", "approval"],
    "negative": ["fear", "nervousness", "remorse", "embarrassment", "disappointment", "sadness", "grief", "disgust",
                 "anger", "annoyance", "disapproval"],
    "ambiguous": ["realization", "surprise", "curiosity", "confusion", "neutral"]
}

"""First, let's extract all emotions from the each example and store them in a list."""

train_df['list of emotions'] = train_df['emotion'].apply(lambda x: x.split(','))
test_df['list of emotions'] = test_df['emotion'].apply(lambda x: x.split(','))
valid_df['list of emotions'] = valid_df['emotion'].apply(lambda x: x.split(','))

"""We can then apply index to class mapping to get the class labels for each row"""

train_df['emotion'] = train_df['list of emotions'].apply(lambda x: idx2class(x))
test_df['emotion'] = test_df['list of emotions'].apply(lambda x: idx2class(x))
valid_df['emotion'] = valid_df['list of emotions'].apply(lambda x: idx2class(x))

"""Finally, we can reduce the number of classes to 7 by using the EmotionMapping function."""

train_df['ekman_emotion'] = train_df['emotion'].apply(lambda x: EmotionMapping(x))
test_df['ekman_emotion'] = test_df['emotion'].apply(lambda x: EmotionMapping(x))
valid_df['ekman_emotion'] = valid_df['emotion'].apply(lambda x: EmotionMapping(x))

train_df.head(10)


def clean_text(text):
    """
    This function cleans the text in the dataframe and returns a list of cleaned text.
    text: a string

    return: modified initial string
    """
    # Removing Emojis
    text = emoji.demojize(text)  # remove emojis
    text = str(text).lower()  # text to lower case
    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)  # remove punctuation
    return text


"""One hot encoding of emotions """

for i in ekman_map:
    train_df[i] = train_df['ekman_emotion'].apply(lambda x: 1 if i in x else 0)
    test_df[i] = test_df['ekman_emotion'].apply(lambda x: 1 if i in x else 0)
    valid_df[i] = valid_df['ekman_emotion'].apply(lambda x: 1 if i in x else 0)

train_df.head(10)

"""### 2.1 Base model config

#### Computing max length of samples

`max_length` variable is used to limit the length of the input text that is fed to the model. The sequence will be padded with the `<PAD>` token if the length of the sequence is less than `max_length` and the sequence will be truncated if the length of the sequence is more than `max_length`. This is done to ensure that the model can handle any size of input text.
"""

full_text = pd.concat([train_df['text'], valid_df['text'], test_df['text']])
max_length = full_text.apply(lambda x: len(x.split())).max()
max_length

"""I am going to use Google's BERT base model which contains 110M parameters."""

model_name = 'bert-base-uncased'
config = BertConfig.from_pretrained(model_name, output_hidden_states=False)
tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path=model_name, config=config)
transformer_model = TFBertModel.from_pretrained(model_name, config=config)

"""### 2.2 Model architecture

model takes three inputs that result from tokenization:

- `input_ids`: indices of input sequence tokens in the vocabulary
- `token_type_ids`: Segment token indices to indicate first and second portions of the inputs. 0 for sentence A and 1 for sentence B
- `attention mask`: Mask to avoid performing attention on padding token indices. 0 for masked and 1 for not masked

I have a sigmoided output layer in the model because it is more appropriate than a softmax layer. This is because I are trying to predict the probability of each label and not the label itself.
"""


def my_model(n_labels):
    # Load the MainLayer
    bert = transformer_model.layers[0]

    ## INPUTS
    input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')
    attention_mask = Input(shape=(max_length,), name='attention_mask', dtype='int32')
    token_type_ids = Input(shape=(max_length,), name='token_type_ids', dtype='int32')
    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}

    ## LAYERS
    bert_model = bert(inputs)[1]
    dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')
    pooled_output = dropout(bert_model, training=False)

    ## OUTPUT
    emotion = Dense(units=n_labels, activation='sigmoid',
                    kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='emotion')(pooled_output)
    outputs = emotion

    model = Model(inputs=inputs, outputs=outputs, name='BERT_Emotion_Classifier')

    return model


model = my_model(len(ekman_map))
model.summary()

plot_model(model, show_shapes=True, dpi=300)

"""### 2.3 Data tokenization"""

## Train 
x_train = train_df['text']
y_train = train_df.loc[:, ekman_map.keys()].values

train_tokenized = tokenizer(
    text=list(x_train),
    add_special_tokens=True,
    max_length=max_length,
    padding='max_length',
    truncation=True,
    return_tensors='tf',
    return_attention_mask=True,
    return_token_type_ids=True
)

# pickle.dump(train_tokenized, open("/content/drive/MyDrive/Variables/train_tokenized.pkl", 'wb'))

# with open("/content/drive/MyDrive/Variables/train_tokenized.pkl", 'rb') as train_file:
#     train_tokenized = pickle.load(train_file)

## Test
x_test = test_df['text']
y_test = test_df.loc[:, ekman_map.keys()].values

test_tokenized = tokenizer(
    text=list(x_test),
    add_special_tokens=True,
    max_length=max_length,
    padding='max_length',
    truncation=True,
    return_tensors='tf',
    return_attention_mask=True,
    return_token_type_ids=True
)

# pickle.dump(test_tokenized, open("/content/drive/MyDrive/Variables/test_tokenized.pkl", 'wb'))

# with open("/content/drive/MyDrive/Variables/train_tokenized.pkl", 'rb') as test_file:
#     test_tokenized = pickle.load(test_file)

## Validation
x_valid = valid_df['text']
y_valid = valid_df.loc[:, ekman_map.keys()].values

valid_tokenized = tokenizer(
    text=list(x_valid),
    add_special_tokens=True,
    max_length=max_length,
    padding='max_length',
    truncation=True,
    return_tensors='tf',
    return_attention_mask=True,
    return_token_type_ids=True
)

# pickle.dump(valid_tokenized, open("/content/drive/MyDrive/Variables/valid_tokenized.pkl", 'wb'))

# with open("/content/drive/MyDrive/Variables/valid_tokenized.pkl", 'rb') as valid_file:
#     valid_tokenized = pickle.load(valid_file)

"""### 2.4 Creating BERT compatible inputs"""

tf_train = {'input_ids': train_tokenized['input_ids'], 'attention_mask': train_tokenized['attention_mask'],
            'token_type_ids': train_tokenized['token_type_ids']}
tf_test = {'input_ids': test_tokenized['input_ids'], 'attention_mask': test_tokenized['attention_mask'],
           'token_type_ids': test_tokenized['token_type_ids']}
tf_valid = {'input_ids': valid_tokenized['input_ids'], 'attention_mask': valid_tokenized['attention_mask'],
            'token_type_ids': valid_tokenized['token_type_ids']}

train = tf.data.Dataset.from_tensor_slices((tf_train, y_train)).batch(80)
valid = tf.data.Dataset.from_tensor_slices((tf_valid, y_valid)).batch(80)
test = tf.data.Dataset.from_tensor_slices((tf_test, y_test)).batch(80)

from tensorflow.keras import backend as K

K.clear_session()

"""Prior experiments with BERT showed that the model starts to overfit after ~2 epochs and Tanh performed significantly worse than sigmoid.

# 3. Evaluation

When dealing with unbalanced data, it is essential to mini-batch train the model instead of training it on all the data. This helps to prevent the model from overfitting the minority class. It is also essential to be thoughtful about what metric is being used for model evaluation. When dealing with unbalanced data, accuracy is not a good metric, as the model can predict the majority class every time and still have high accuracy. Instead, it is crucial to use the precision/recall or the F1 score, as these metrics consider false positives and false negatives.
"""

model = my_model(len(ekman_map))
model.load_weights('Hapiness_Meter_one.h5')

THRESHOLD = 0.83

# y_pred = model.predict(test)
# import pickle
# pickle.dump(y_pred, open("y_pred.pkl", 'wb'))
# model.save_weights('/content/drive/MyDrive/model/threshold.h5')
# import pickle
# # pickle.dump(y_pred, open("y_pred.pkl", 'wb'))
with open("y_pred.pkl", 'rb') as ypred_file:
    y_pred = pickle.load(ypred_file)

probabilities = y_pred

probabilities = pd.DataFrame(probabilities, columns=ekman_map.keys())
probabilities.index = x_test
probabilities.reset_index(inplace=True)
probabilities.head(10)

y_pred = np.where(y_pred > THRESHOLD, 1, 0)

recall = []
f1 = []
precision = []
emotions = ekman_map.keys()

for i in range(len(emotions)):
    f1.append(f1_score(y_test[:, i], y_pred[:, i], average='macro'))
    precision.append(precision_score(y_test[:, i], y_pred[:, i], average='macro'))

results = pd.DataFrame({'precision': precision, 'f1': f1})
results.index = emotions

means = {'precision': np.mean(precision), 'f1': np.mean(f1)}
means = pd.DataFrame(means, index=['mean'])

pd.concat([results, means], axis=0)

"""### 3.1 Optimization

Finding the best value of Threshold. I chose f1-score as the main metric because it is more robust than precision and recall alone.
"""

best_threshold = 0
best_precision = 0
# pred = model.predict(test)
# pickle.dump(pred, open("pred.pkl", 'wb'))
with open("pred.pkl", 'rb') as pred_file:
    pred = pickle.load(pred_file)

for threshold in np.arange(0.30, 0.99, 0.01):
    preds = np.where(pred > threshold, 1, 0)

    precision = precision_score(y_test, preds, average='macro', zero_division=0)

    if precision > best_precision:
        best_threshold = threshold
        best_precision = precision
    else:
        continue

print(f'Best threshold: {best_threshold}\nBest precision: {best_precision}')

THRESHOLD = 0.39

"""## 4. Make Predictions"""


def pred(text, model, THRESHOLD):
    text = [clean_text(text) for text in text]

    tokenized = tokenizer(
        text=text,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='tf',
        return_attention_mask=True,
        return_token_type_ids=True
    )

    tf_test = {'input_ids': tokenized['input_ids'], 'attention_mask': tokenized['attention_mask'],
               'token_type_ids': tokenized['token_type_ids']}

    pred = model.predict(tf_test)

    probabilities = pred
    probabilities = pd.DataFrame(probabilities * 100, columns=ekman_map.keys())
    probabilities.index = text
    probabilities.reset_index(inplace=True)

    pred = np.where(pred > THRESHOLD, 1, 0)

    pred = pd.DataFrame(pred, columns=ekman_map.keys())
    pred['emotion'] = pred.iloc[:, 1:].idxmax(axis=1)
    pred.drop(columns=emotions, inplace=True)
    pred.index = text
    pred.reset_index(inplace=True)

    return pred, probabilities


result, probabilities = pred(["very disappointed with the construction quality"], model, THRESHOLD)

result

probabilities

print("Getting Emails Data:")

data = pd.read_excel("Customer_Mails.xlsx")
data

data['Body.TextBody'] = data['Body.TextBody'].astype(str)
data['Body.TextBody'] = data['Body.TextBody'].replace("_x000D_\n", " ", regex=True)
print("done")

print("Passing the mails through model:")

result, probabilities = pred(data.iloc[:, 8], model, THRESHOLD)

probabilities['anger'] = np.where(probabilities['anger'].between(1, 1.4), probabilities['anger'] * 70,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(1.4, 1.9), probabilities['anger'] * 55,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(1.9, 2.3), probabilities['anger'] * 40,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(2.3, 2.7), probabilities['anger'] * 35,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(2.7, 3), probabilities['anger'] * 30,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(3, 4), probabilities['anger'] * 24,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(4, 5), probabilities['anger'] * 19,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(5, 7), probabilities['anger'] * 13,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(9, 10), probabilities['anger'] * 9,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(10, 15), probabilities['anger'] * 6,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(16, 20), probabilities['anger'] * 4,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(21, 25), probabilities['anger'] * 3,
                                  probabilities['anger'])

probabilities['anger'] = np.where(probabilities['anger'].between(26, 40), probabilities['anger'] * 2,
                                  probabilities['anger'])

##sadness
probabilities['sadness'] = np.where(probabilities['sadness'].between(3, 4), probabilities['sadness'] * 24,
                                    probabilities['sadness'])

probabilities['sadness'] = np.where(probabilities['sadness'].between(4, 5), probabilities['sadness'] * 19,
                                    probabilities['sadness'])

probabilities['sadness'] = np.where(probabilities['sadness'].between(5, 7), probabilities['sadness'] * 13,
                                    probabilities['sadness'])

probabilities['sadness'] = np.where(probabilities['sadness'].between(9, 10), probabilities['sadness'] * 9,
                                    probabilities['sadness'])

probabilities['sadness'] = np.where(probabilities['sadness'].between(10, 15), probabilities['sadness'] * 6,
                                    probabilities['sadness'])

probabilities['sadness'] = np.where(probabilities['sadness'].between(16, 20), probabilities['sadness'] * 4,
                                    probabilities['sadness'])

probabilities['sadness'] = np.where(probabilities['sadness'].between(21, 25), probabilities['sadness'] * 3,
                                    probabilities['sadness'])

probabilities['sadness'] = np.where(probabilities['sadness'].between(26, 40), probabilities['sadness'] * 2,
                                    probabilities['sadness'])

probabilities["emotion"] = probabilities.iloc[:, 1:8].idxmax(axis="columns")

probabilities['emotion'] = np.where(probabilities['anger'] > 40,
                                    probabilities['emotion'].replace('neutral', 'anger', regex=True),
                                    probabilities['emotion'])

probabilities['emotion'] = np.where(probabilities['anger'] > 40,
                                    probabilities['emotion'].replace('joy', 'anger', regex=True),
                                    probabilities['emotion'])

probabilities['emotion'] = np.where(probabilities['anger'] > 40,
                                    probabilities['emotion'].replace('surprise', 'anger', regex=True),
                                    probabilities['emotion'])

probabilities['emotion'] = np.where(probabilities['sadness'] > 5,
                                    probabilities['emotion'].replace('neutral', 'sadness', regex=True),
                                    probabilities['emotion'])

probabilities['emotion'] = np.where(probabilities['sadness'] > 5,
                                    probabilities['emotion'].replace('joy', 'sadness', regex=True),
                                    probabilities['emotion'])

probabilities['emotion'] = np.where(probabilities['sadness'] > 60,
                                    probabilities['emotion'].replace('anger', 'sadness', regex=True),
                                    probabilities['emotion'])

probabilities

print("done")

probabilities['emotion'].value_counts()

probabilities.drop(columns="index")

data = data.join(probabilities)
data

print("Performing Model 2 of analyzing categories:")

##Abhishek Model
from keras_preprocessing import text, sequence
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Activation, Dropout, Dense
from sklearn.model_selection import train_test_split
from numpy.random import seed

seed(1)
tf.random.set_seed(2)

train_df = pd.read_csv("newtrain.csv", encoding='windows-1252')

y_pandas_df = pd.get_dummies(train_df['label'])
print(type(y_pandas_df))
print(y_pandas_df.shape)
print(y_pandas_df.ndim)
y_pandas_df.head()

y = y_pandas_df.values
print("properties of y")
print(
    "type : {}, dimensions : {}, shape : {}, total no. of elements : {}, data type of each element: {}, size of each element {} bytes".format(
        type(y), y.ndim, y.shape, y.size, y.dtype, y.itemsize))

train_df = train_df.drop(['label'], axis=1)

train_df.shape
x = train_df.values.flatten()

list_of_classes = ['Construction Quality', 'Escalation/Brand', 'Legal', 'Others/Social Media',
                   'Payments/Interest waiver']
max_features = 20000
max_text_length = 500
embedding_dims = 50
batch_size = 20
epochs = 14
num_filters_1 = 250
num_filters_2 = 250
filter_size = 1

x_tokenizer = text.Tokenizer(num_words=max_features)
x_tokenizer.fit_on_texts(list(x))
x_tokenized = x_tokenizer.texts_to_sequences(x)
x_train_val = sequence.pad_sequences(x_tokenized, maxlen=max_text_length)

test_df = pd.read_excel('Customer_Mails.xlsx')
test_df['Body.TextBody'] = test_df['Body.TextBody'].astype(str)  # dimensions and headers Need to be udated accordingly
test_df['Body.TextBody'] = test_df['Body.TextBody'].replace("_x000D_\n", "", regex=True)
test_df

x_test = test_df['Body.TextBody'].values
x_test_tokenized = x_tokenizer.texts_to_sequences(x_test)
x_testing = sequence.pad_sequences(x_test_tokenized, maxlen=max_text_length)

from tensorflow.python.keras.models import load_model

model2 = load_model("Abhishek_Model.h5")
y_testing = model2.predict(x_testing)
max_idx = y_testing.argmax(axis=1)  # get the indexes for the max probabilities
out_labels = [list_of_classes[i] for i in max_idx]
df = pd.DataFrame(y_testing,
                  columns=list_of_classes)

df1 = pd.DataFrame(data=out_labels)

C = df.join(df1)

dashboard = data.join(C)
dashboard = dashboard.drop(columns="index")
dashboard

print("Generating Final Output: ")

dashboard.to_csv("Dashboard.csv")

print("done")

print("Creating FTP Server Connection:")

# FTP Connection

import ftplib

# connection parameters
ftpHost = '115.124.113.164'
ftpPort = 21
ftpUsername = 'cce'
ftpPassword = 'Outdo2022'

# create an FTP client instance, use the timeout(seconds) only if connectio is slow
ftp = ftplib.FTP(timeout=1200)

# connecting to ftp server
ftp.connect(ftpHost, ftpPort)

# log in to ftp server
ftp.login(ftpUsername, ftpPassword)

print("Connected.")

ftp.nlst()

ftp.cwd("Dashboard/")

ftp.nlst()

ftp.sendcmd('PASV')

print("Pushing the data to FTP server:")

localfile = open("Dashboard.csv", 'rb')
ftp.storbinary('APPE Dashboard.csv', localfile)
localfile.close()

print(ftp.close())

print("Model Run Successfully ")
